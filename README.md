# E-Commerce Analytics Data Pipeline (Azure Databricks)

An end-to-end **E-Commerce Analytics Platform** built on **Azure Data Lake + Azure Databricks** using the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**.
The curated Gold layer powers **Power BI dashboards** for sales, customer, and operational analytics.

---

## ğŸ“Œ Project Overview

This project demonstrates a **scalable, production-grade data engineering pipeline** for an e-commerce platform (**ShopVista**) using modern lakehouse principles.

**Key highlights:**

* Medallion architecture (Bronze, Silver, Gold)
* Dimensional + Fact modeling
* Databricks ETL/ELT with Unity Catalog
* Secure access to Azure Data Lake (ADLS Gen2)
* Power BI semantic layer and dashboards

---

## ğŸ—ï¸ Architecture

### High-Level Pipeline Architecture

![Pipeline Architecture](project_architecture.png)

**Flow:**

1. **Source System (ShopVista)**

   * Operational data exported as CSV files
2. **Azure Data Lake Storage (ADLS Gen2)**

   * Centralized raw storage
3. **Azure Databricks + Unity Catalog**

   * ETL/ELT processing
   * Data governance and access control
4. **Medallion Layers**

   * **Bronze**: Raw ingestion
   * **Silver**: Cleaned, standardized data
   * **Gold**: Aggregated, analytics-ready tables
5. **Power BI**

   * Reporting & analytics layer

---

## ğŸ“Š Analytics Dashboard

### E-Commerce Executive Dashboard (Power BI)

![E-Commerce Dashboard](ecommerce_analytics_report.jpg)

**Key Metrics:**

* Total Sales & Units Sold
* Repeat Customer Rate
* Customer Count by Region
* Revenue Trends (Monthly)
* Channel Performance (Mobile vs Website)
* Brand & Category Sales Contribution

---

## ğŸ—‚ï¸ Repository Structure

```text
e-commerce-data-pipeline/
â”‚
â”œâ”€â”€ 1_Setup/
â”‚   â”œâ”€â”€ setup-raw.py
â”‚   â””â”€â”€ setup_catalog.py
â”‚
â”œâ”€â”€ 2_medallion_processing_dim/
â”‚   â”œâ”€â”€ 1_dim_bronze.py
â”‚   â”œâ”€â”€ 2_dim_silver.py
â”‚   â””â”€â”€ 3_dim_gold.py
â”‚
â”œâ”€â”€ 3_medallion_processing_fact/
â”‚   â”œâ”€â”€ 1_brnz_fact.py
â”‚   â”œâ”€â”€ 1_brnz_fact_shipments.py
â”‚   â”œâ”€â”€ 1_brnz_fact_ordr_rturn.py
â”‚   â”œâ”€â”€ 2_slvr_fact.py
â”‚   â”œâ”€â”€ 2_slvr_fact_ordr_shipments.py
â”‚   â”œâ”€â”€ 2_slvr_fact_ordr_rturn.py
â”‚   â”œâ”€â”€ 3_gold_fact.py
â”‚   â”œâ”€â”€ 3_gold_fact_ordr_shipments.py
â”‚   â”œâ”€â”€ 3_gold_fact_ordr_rturn.py
â”‚   â”œâ”€â”€ 4_daily_summary.py
â”‚   â”œâ”€â”€ 4_monthly_order_shipment_summary.py
â”‚   â””â”€â”€ 4_monthly_order_return_summary.py
â”‚
â””â”€â”€ manifest.mf
```

---

## ğŸ§± Medallion Architecture Details

### ğŸ¥‰ Bronze Layer

* Raw ingestion from CSV files
* Minimal transformations
* Schema enforcement
* Audit columns (ingestion timestamp, source)

**Scripts:**

* `1_dim_bronze.py`
* `1_brnz_fact*.py`

---

### ğŸ¥ˆ Silver Layer

* Data cleansing & standardization
* Deduplication
* Business rule validation
* Referential integrity between facts & dimensions

**Scripts:**

* `2_dim_silver.py`
* `2_slvr_fact*.py`

---

### ğŸ¥‡ Gold Layer

* Aggregated, analytics-ready tables
* Star schema aligned
* Optimized for Power BI consumption

**Outputs include:**

* Sales facts
* Order, shipment & return summaries
* Daily & monthly KPIs

**Scripts:**

* `3_dim_gold.py`
* `3_gold_fact*.py`
* `4_*_summary.py`

---

## âš™ï¸ Setup & Deployment

### Prerequisites

* Azure Subscription
* Azure Data Lake Storage Gen2
* Azure Databricks (with Unity Catalog enabled)
* Power BI Desktop / Power BI Service

---

### Step 1: Environment Setup

Run the setup scripts in Databricks:

```bash
1_Setup/setup-raw.py
1_Setup/setup_catalog.py
```

This will:

* Create catalogs, schemas, and external locations
* Configure Unity Catalog permissions

---

### Step 2: Bronze Ingestion

Run Bronze notebooks/scripts to ingest raw CSV data into ADLS-backed tables.

---

### Step 3: Silver Transformation

Execute Silver layer scripts to clean and standardize data.

---

### Step 4: Gold Aggregation

Run Gold layer scripts to generate analytical tables and summaries.

---

### Step 5: Power BI Integration

* Connect Power BI to Databricks SQL / Gold tables
* Build semantic model
* Publish dashboards

---

## ğŸ” Security & Governance

* Unity Catalog for:

  * Table-level and column-level security
  * Centralized data governance
* Secure ADLS access via managed identity / access connector

---

## ğŸš€ Future Enhancements

* Incremental loads using Delta Lake
* CI/CD with Azure DevOps / GitHub Actions
* Data quality checks with expectations
* Real-time ingestion (Event Hub / Kafka)
* Advanced customer segmentation & ML models

---

## ğŸ‘¤ Author

**Divya Pathak**
*Data Engineer | Azure | Databricks | Power BI*

---

## ğŸ“„ License

This project is for **educational and portfolio purposes**.
You are free to adapt and extend it for learning or internal use.

---

â­ If you find this project useful, donâ€™t forget to **star the repository**!
